{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# NVIDIA NIM Config",
   "id": "2e81ef1d3e63598"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-23T05:52:20.542006Z",
     "start_time": "2025-02-23T05:52:19.746451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "import time\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem, TableItem\n",
    "from docling.datamodel.base_models import FigureElement, InputFormat, Table\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "\n",
    "# 从文件中读取 API key\n",
    "def read_api_keys(file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Reads the API keys from the specified file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file containing the API keys.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the API keys.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            api_keys = {\n",
    "                \"NVIDIA_API_KEY\": lines[0].strip().strip('[\"').strip('\"]'),\n",
    "                \"TTS_APIKEY\": lines[1].strip().strip(\"',\").strip(\"']\")\n",
    "            }\n",
    "        return api_keys\n",
    "    except FileNotFoundError:\n",
    "        raise Exception(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An error occurred while reading the API keys: {str(e)}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "api_keys = read_api_keys('./nv_api_key.txt')\n",
    "NV_API_KEY = api_keys[\"NVIDIA_API_KEY\"]\n",
    "TTS_API_KEY = api_keys[\"TTS_APIKEY\"]\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"  # conda environments have to set this\n",
    "os.environ[\"NVIDIA_API_KEY\"] = NV_API_KEY\n",
    "ChatNVIDIA.get_available_models()"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='nvidia/llama-3.1-nemotron-70b-reward', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-51b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='databricks/dbrx-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-dbrx-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/codegemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-8b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mathstral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama2-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama2-70b', 'playground_llama2_70b', 'llama2_70b', 'playground_llama2_13b', 'llama2_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-70b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='upstage/solar-10.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-solar-10_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='aisingapore/sea-lion-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-sea-lion-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.2-1b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mamba-codestral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='seallms/seallm-7b-v2.5', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-seallm-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.3', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v03'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='tokyotech-llm/llama-3-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='yentinglin/llama-3-taiwan-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/codegemma-1.1-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-1.1-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/paligemma', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/paligemma', aliases=['ai-google-paligemma'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mediatek/breeze-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-breeze-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-coder-6.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-deepseek-coder-6_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-vision-128k-instruct', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct', aliases=['ai-phi-3-vision-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ai21labs/jamba-1.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='01-ai/yi-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-yi-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-34b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-34b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ai21labs/jamba-1.5-large-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-moe-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/vila', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/vila', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-4k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/usdcode-llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-2-9b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-9b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='rakuten/rakutenai-7b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-coder-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen2.5-coder-32b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='baichuan-inc/baichuan2-13b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/codellama-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codellama-70b', 'playground_llama2_code_70b', 'llama2_code_70b', 'playground_llama2_code_34b', 'llama2_code_34b', 'playground_llama2_code_13b', 'llama2_code_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='zyphra/zamba2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-3.0-3b-a800m-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.2-3b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='meta/llama-3.2-90b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-90b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.2-11b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/usdcode-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/mistral-nemo-minitron-8b-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-8k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='writer/palmyra-fin-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/neva-22b', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b', aliases=['ai-neva-22b', 'playground_neva_22b', 'neva_22b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='rakuten/rakutenai-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/recurrentgemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-recurrentgemma-2b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-3.0-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-340b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['qa-nemotron-4-340b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='google/gemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-mini-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-2-2b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b-32k'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-8b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-8b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='adept/fuyu-8b', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b', aliases=['ai-fuyu-8b', 'playground_fuyu_8b', 'fuyu_8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-mini-hindi-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='microsoft/kosmos-2', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2', aliases=['ai-microsoft-kosmos-2', 'playground_kosmos_2', 'kosmos_2'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='thudm/chatglm3-6b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='qwen/qwen2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large-2-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='abacusai/dracarys-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='snowflake/arctic', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-arctic'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='google/gemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2b', 'playground_gemma_2b', 'gemma_2b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/codestral-22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codestral-22b-instruct-v01'], supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='google/gemma-2-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-27b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/deplot', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/deplot', aliases=['ai-google-deplot', 'playground_deplot', 'deplot'], supports_tools=False, supports_structured_output=False, base_model=None)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Convert PDF to Markdown",
   "id": "f31ba051500ad5b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:52:20.561652Z",
     "start_time": "2025-02-23T05:52:20.551644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "_log = logging.getLogger(__name__)\n",
    "IMAGE_RESOLUTION_SCALE = 2.0\n",
    "markdown_file_path = None\n",
    "output_dir = None\n",
    "\n",
    "\n",
    "def save_page_images(conv_res, output_dir, doc_filename):\n",
    "    \"\"\"Save images of each page.\"\"\"\n",
    "    for page_no, page in conv_res.document.pages.items():\n",
    "        page_no = page.page_no\n",
    "        page_image_filename = output_dir / f\"{doc_filename}-{page_no}.png\"\n",
    "        with page_image_filename.open(\"wb\") as fp:\n",
    "            page.image.pil_image.save(fp, format=\"PNG\")\n",
    "\n",
    "\n",
    "def save_figures_and_tables(conv_res, output_dir, doc_filename):\n",
    "    \"\"\"Save images of figures and tables.\"\"\"\n",
    "    table_counter = 0\n",
    "    picture_counter = 0\n",
    "    for element, _level in conv_res.document.iterate_items():\n",
    "        if isinstance(element, TableItem):\n",
    "            table_counter += 1\n",
    "            element_image_filename = (\n",
    "                    output_dir / f\"{doc_filename}-table-{table_counter}.png\"\n",
    "            )\n",
    "            with element_image_filename.open(\"wb\") as fp:\n",
    "                element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "\n",
    "        if isinstance(element, PictureItem):\n",
    "            picture_counter += 1\n",
    "            element_image_filename = (\n",
    "                    output_dir / f\"{doc_filename}-picture-{picture_counter}.png\"\n",
    "            )\n",
    "            with element_image_filename.open(\"wb\") as fp:\n",
    "                element.get_image(conv_res.document).save(fp, \"PNG\")\n",
    "\n",
    "\n",
    "def save_markdown_with_embedded_pictures(conv_res, output_dir, doc_filename):\n",
    "    \"\"\"Save markdown with embedded pictures.\"\"\"\n",
    "    md_filename = output_dir / f\"{doc_filename}-with-images.md\"\n",
    "    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.EMBEDDED)\n",
    "\n",
    "\n",
    "def save_markdown_with_externally_referenced_pictures(conv_res, output_dir, doc_filename):\n",
    "    \"\"\"Save markdown with externally referenced pictures.\"\"\"\n",
    "    md_filename = output_dir / f\"{doc_filename}-with-image-refs.md\"\n",
    "    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "\n",
    "def convert_pdf_to_markdown(input_doc_path: str):\n",
    "    global markdown_file_path\n",
    "    global output_dir\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    input_doc_path = Path(f\"{input_doc_path}\")\n",
    "    output_dir = Path(f\"{input_doc_path.stem}-{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\")\n",
    "    markdown_storage_dir = Path(f\"data/markdown/{output_dir}\")\n",
    "    markdown_file_path = markdown_storage_dir / f\"{input_doc_path.stem}-with-image-refs.md\"\n",
    "\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "    pipeline_options.generate_page_images = True\n",
    "    pipeline_options.generate_picture_images = True\n",
    "\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    conv_res = doc_converter.convert(input_doc_path)\n",
    "\n",
    "    markdown_storage_dir.mkdir(parents=True, exist_ok=True)\n",
    "    doc_filename = conv_res.input.file.stem\n",
    "\n",
    "    save_page_images(conv_res, markdown_storage_dir, doc_filename)\n",
    "    save_figures_and_tables(conv_res, markdown_storage_dir, doc_filename)\n",
    "    save_markdown_with_embedded_pictures(conv_res, markdown_storage_dir, doc_filename)\n",
    "    save_markdown_with_externally_referenced_pictures(conv_res, markdown_storage_dir, doc_filename)\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    _log.info(f\"Document converted and figures exported in {end_time:.2f} seconds.\")\n",
    "    print(f\"Markdown file saved at {markdown_file_path}\")"
   ],
   "id": "2db9ce68513bea31",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FAISS Config",
   "id": "1533a3ca3c3bc0d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:52:20.586596Z",
     "start_time": "2025-02-23T05:52:20.577870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from typing import Tuple, List\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "faiss_store_path_PDF = Path(\"data/faiss_store/PDF\")\n",
    "store_PDF = None\n",
    "retriever_PDF = None\n",
    "\n",
    "faiss_store_path_chart = Path(\"data/faiss_store/chart\")\n",
    "store_chart = None\n",
    "retriever_chart = None\n",
    "\n",
    "\n",
    "def read_markdown_content(file_path: Path) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Read and filter markdown content from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the markdown file\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - List[str]: Filtered content lines\n",
    "            - List[str]: Corresponding source paths for each line\n",
    "    \"\"\"\n",
    "    data: List[str] = []\n",
    "    sources: List[str] = []\n",
    "\n",
    "    try:\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:  # Check for non-empty lines\n",
    "                    data.append(line)\n",
    "                    sources.append(str(file_path))\n",
    "\n",
    "        print(f\"Read {len(data)} lines from {file_path}\")\n",
    "        return data, sources\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return [], []\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return [], []\n",
    "\n",
    "\n",
    "# # Usage\n",
    "# data, sources = read_markdown_content(markdown_file_path)\n",
    "# documents = data  # Since we already filtered empty lines during reading\n",
    "\n",
    "def create_faiss_store(\n",
    "        documents: List[str],\n",
    "        sources: List[str],\n",
    "        store_path: Path,\n",
    "        chunk_size: int = 400,\n",
    "        model: str = \"NV-Embed-QA\"\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Create and save a FAISS vector store from documents.\n",
    "\n",
    "    Args:\n",
    "        documents: List of document texts\n",
    "        sources: List of source paths\n",
    "        store_path: Path to save the FAISS store\n",
    "        chunk_size: Size of text chunks for splitting\n",
    "        model: NVIDIA embedding model name\n",
    "\n",
    "    Returns:\n",
    "        FAISS: The created FAISS store\n",
    "    \"\"\"\n",
    "    embedder = NVIDIAEmbeddings(model=model)\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, separator=\" \")\n",
    "\n",
    "    docs: List[str] = []\n",
    "    metadatas: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "        splits = text_splitter.split_text(doc)\n",
    "        docs.extend(splits)\n",
    "        metadatas.extend([{\"source\": sources[i]} for _ in splits])\n",
    "\n",
    "    store = FAISS.from_texts(docs, embedder, metadatas=metadatas)\n",
    "    store.save_local(store_path)\n",
    "    return store\n",
    "\n",
    "# # Example usage\n",
    "# documents = [\"Document 1 text\", \"Document 2 text\"]\n",
    "# sources = [\"source1.md\", \"source2.md\"]\n",
    "# store_path = Path(\"data/faiss_store/example\")\n",
    "# \n",
    "# store_PDF = create_faiss_store(documents, sources, store_path)\n",
    "\n"
   ],
   "id": "3284ea6dbb4c0258",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Langchain Config",
   "id": "680fdca9ef354b1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:52:20.602800Z",
     "start_time": "2025-02-23T05:52:20.594720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")\n",
    "\n",
    "article_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert IELTS English teacher. Based on the provided context, help the student understand the article. \"\n",
    "            \"Explain difficult vocabulary, summarize key points, and provide examples where necessary. \"\n",
    "            \"Use clear and concise language. \"\n",
    "            \"Answer solely based on the following context:\\n<Documents>\\n{context}\\n</Documents>\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"请帮助我理解文章的主要内容和难词。\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "podcast_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert IELTS English teacher and podcast scriptwriter. Based on the provided context, create a podcast script for the student. \"\n",
    "            \"The script should be engaging, colloquial, and suitable for an IELTS proficiency level. \"\n",
    "            \"Focus on summarizing the key points of the article, explaining difficult vocabulary, and providing examples where necessary. \"\n",
    "            \"Do not include any timestamps, segment labels, or additional notes. \"\n",
    "            \"The script should be approximately 4 minutes long when spoken. \"\n",
    "            \"Start the script with '<podcast>' and end it with '</podcast>'. \"\n",
    "            \"Answer solely based on the following context:\\n<Documents>\\n{context}\\n</Documents>\"\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"请帮助我生成一个符合雅思能力水平的播客节目文稿，时长约为4分钟。\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "chart_analysis_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a data analysis expert and IELTS academic writing instructor. Based on the provided chart data, generate a model essay for IELTS Writing Task 1. \"\n",
    "            \"The essay should strictly follow academic writing standards and demonstrate professional chart interpretation skills. \"\n",
    "            \"Required elements:\\n\"\n",
    "            \"1. Precise identification of chart type (line/bar/pie etc.) and main trends\\n\"\n",
    "            \"2. Logical grouping of data features with comparative analysis\\n\"\n",
    "            \"3. Accurate use of statistical terminology and percentage expressions\\n\"\n",
    "            \"4. Grammatical diversity in data description (passive/active voice conversion)\\n\"\n",
    "            \"5. Clear overview paragraph highlighting most significant information\\n\"\n",
    "            \"6. Word count between 150-200 words\\n\"\n",
    "            \"Format requirements:\\n\"\n",
    "            \"- Use <essay> tags to wrap content\\n\"\n",
    "            \"- Avoid markdown formatting\\n\"\n",
    "            \"- Prohibit speculative statements beyond given data\\n\"\n",
    "            \"Reference only the following data:\\n<Chart>\\n{context}\\n</Chart>\"\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"请根据提供的图表数据，生成一篇符合雅思学术写作标准的图表分析范文。\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "\n",
    "def create_chains(model: str = \"meta/llama3-70b-instruct\"):\n",
    "    \"\"\"\n",
    "    Create article and podcast processing chains.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        model: LLM model name\n",
    "\n",
    "    Returns:\n",
    "        tuple: (article_chain, podcast_chain)\n",
    "    \"\"\"\n",
    "\n",
    "    llm = ChatNVIDIA(model=model)\n",
    "    global retriever_PDF\n",
    "    global retriever_chart\n",
    "\n",
    "    if retriever_PDF is None:\n",
    "        article_chain = None\n",
    "        podcast_chain = None\n",
    "    else:\n",
    "        article_chain = (\n",
    "                {\"context\": retriever_PDF, \"question\": RunnablePassthrough()}\n",
    "                | article_prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        podcast_chain = (\n",
    "                {\"context\": retriever_PDF, \"question\": RunnablePassthrough()}\n",
    "                | podcast_prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "        )\n",
    "    if retriever_chart is None:\n",
    "        chart_chain = None\n",
    "    else:\n",
    "        chart_chain = (\n",
    "                {\"context\": retriever_chart, \"question\": RunnablePassthrough()}\n",
    "                | chart_analysis_prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    return article_chain, podcast_chain, chart_chain\n"
   ],
   "id": "365d166f81923e76",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Podcast Config",
   "id": "b4de3fe3225fd0ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:52:20.619572Z",
     "start_time": "2025-02-23T05:52:20.610522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Podcast Logic\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "voice_list = [\n",
    "    \"English-US.Female-1\",\n",
    "    \"English-US.Male-1\",\n",
    "    \"English-US.Female-Neutral\",\n",
    "    \"English-US.Male-Neutral\",\n",
    "    \"English-US.Female-Angry\",\n",
    "    \"English-US.Male-Angry\",\n",
    "    \"English-US.Female-Calm\",\n",
    "    \"English-US.Male-Calm\",\n",
    "    \"English-US.Female-Fearful\",\n",
    "    \"English-US.Female-Happy\",\n",
    "    \"English-US.Male-Happy\",\n",
    "    \"English-US.Female-Sad\"\n",
    "]\n",
    "\n",
    "\n",
    "def split_podcast_text(podcast_text: str, max_length: int = 600) -> list[str]:\n",
    "    \"\"\"\n",
    "    将包含 <podcast> 标签的文本分割为段落列表，并尽量合并段落，确保每个段落字符数不超过 max_length。\n",
    "\n",
    "    Args:\n",
    "        podcast_text (str): 生成的播客文稿文本（包含 <podcast> 标签）\n",
    "        max_length (int): 每个段落的字符数上限，默认为 1000\n",
    "\n",
    "    Returns:\n",
    "        list[str]: 合并后的段落列表，例如 [\"para1\", \"para2\", ...]\n",
    "    \"\"\"\n",
    "    # 1. 去除 <podcast> 标签并提取正文内容\n",
    "    content = podcast_text.strip()  # 去除首尾空格\n",
    "    content = content.replace(\"<podcast>\", \"\").replace(\"</podcast>\", \"\").strip()\n",
    "\n",
    "    # 2. 按换行符分割段落，并过滤空段落\n",
    "    paragraphs = [p.strip() for p in content.split(\"\\n\") if p.strip()]\n",
    "\n",
    "    # 3. 合并段落，确保每个段落字符数不超过 max_length\n",
    "    merged_paragraphs = []\n",
    "    current_paragraph = \"\"\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        # 如果当前段落加上新段落的长度不超过 max_length，则合并\n",
    "        if len(current_paragraph) + len(paragraph) + 1 <= max_length:\n",
    "            current_paragraph += (\" \" + paragraph) if current_paragraph else paragraph\n",
    "        else:\n",
    "            # 如果当前段落不为空，则添加到结果列表\n",
    "            if current_paragraph:\n",
    "                merged_paragraphs.append(current_paragraph)\n",
    "            # 开始新的段落\n",
    "            current_paragraph = paragraph\n",
    "\n",
    "    # 添加最后一个段落\n",
    "    if current_paragraph:\n",
    "        merged_paragraphs.append(current_paragraph)\n",
    "\n",
    "    return merged_paragraphs\n",
    "\n",
    "\n",
    "def synthesize_speech(text: str, voice: str, output: str) -> None:\n",
    "    import time\n",
    "    import wave\n",
    "    import json\n",
    "    import riva.client\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Authentication & service creation\n",
    "    auth = riva.client.Auth(None, True, 'grpc.nvcf.nvidia.com:443',\n",
    "                            [['function-id', '0149dedb-2be8-4195-b9a0-e57e0e14f972'],\n",
    "                             ['authorization',\n",
    "                              f'{TTS_API_KEY}']])\n",
    "    service = riva.client.SpeechSynthesisService(auth)\n",
    "\n",
    "    # Prepare audio file for writing\n",
    "    out_path = Path(output).expanduser()\n",
    "    out_f = wave.open(str(out_path), 'wb')\n",
    "    out_f.setnchannels(1)\n",
    "    out_f.setsampwidth(2)\n",
    "    out_f.setframerate(44100)\n",
    "\n",
    "    try:\n",
    "        print(\"Generating audio for request...\")\n",
    "        start = time.time()\n",
    "        response = service.synthesize(\n",
    "            text, voice, \"en-US\", sample_rate_hz=44100,\n",
    "            audio_prompt_file=None, quality=20, custom_dictionary={}\n",
    "        )\n",
    "        print(f\"Time spent: {(time.time() - start):.3f}s\")\n",
    "        out_f.writeframesraw(response.audio)\n",
    "    finally:\n",
    "        out_f.close()\n",
    "\n",
    "\n",
    "def combine_audio_files(input_dir: str, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Combine all WAV files in the specified directory into a single audio file.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): The directory containing the WAV files to combine.\n",
    "        output_file (str): The path to the output combined audio file.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    wav_files = sorted(input_path.glob(\"podcast_audio-*.wav\"))\n",
    "\n",
    "    combined_audio = []\n",
    "    for wav_file in wav_files:\n",
    "        data, samplerate = sf.read(wav_file)\n",
    "        combined_audio.extend(data)\n",
    "\n",
    "    combined_audio = np.array(combined_audio)\n",
    "    sf.write(output_file, combined_audio, samplerate)\n",
    "\n",
    "# # Example usage\n",
    "# combine_audio_files(\"./data/podcast\", \"./data/podcast/combined_podcast.wav\")"
   ],
   "id": "19c6244fda6dd9d7",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Gradio Config",
   "id": "b8864620fb75ed1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T05:52:20.646628Z",
     "start_time": "2025-02-23T05:52:20.627735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "import base64\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import gradio as gr\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from typing import List\n",
    "\n",
    "TARGET_DIR_PDF = \"data/pdf\"\n",
    "pdf_file_path = \"\"\n",
    "podcast_text_plain = \"\"\n",
    "podcast_text_split = []\n",
    "\n",
    "TARGET_DIR_CHART = \"data/charts\"\n",
    "chart_file_path = \"\"\n",
    "markdown_file_path_chart = \"\"\n",
    "cur_chart_dir = \"\"\n",
    "\n",
    "\n",
    "def save_pdf(file, progress=gr.Progress()):\n",
    "    progress(0, desc=\"Starting task...\")\n",
    "    if file is None:\n",
    "        return \"No file uploaded\", None, None\n",
    "\n",
    "    # 确保目标目录存在\n",
    "    if not os.path.exists(TARGET_DIR_PDF):\n",
    "        os.makedirs(TARGET_DIR_PDF)\n",
    "\n",
    "    try:\n",
    "        # 获取文件名\n",
    "        file_name = os.path.basename(file.name)\n",
    "        # 目标文件路径\n",
    "        target_path = os.path.join(TARGET_DIR_PDF, file_name)\n",
    "        global pdf_file_path\n",
    "        pdf_file_path = target_path\n",
    "        print(pdf_file_path)\n",
    "        # 复制文件到目标目录\n",
    "        shutil.copy(file.name, target_path)\n",
    "        progress(0.20, desc=\"File uploaded successfully!\")\n",
    "        convert_pdf_to_markdown(f\"{pdf_file_path}\")\n",
    "        progress(0.40, desc=\"PDF converted to markdown!\")\n",
    "        data, sources = read_markdown_content(markdown_file_path)\n",
    "        global store_PDF\n",
    "        store_PDF = create_faiss_store(\n",
    "            documents=data,\n",
    "            sources=sources,\n",
    "            store_path=faiss_store_path_PDF\n",
    "        )\n",
    "        progress(0.75, desc=\"Faiss Create successfully!\")\n",
    "        global retriever_PDF\n",
    "        retriever_PDF = store_PDF.as_retriever()\n",
    "        progress(1, desc=\"Embedding and Faiss store created!\")\n",
    "        alert_message = \"Embedding and Faiss completed! You can start asking questions now.\"\n",
    "        return (gr.update(visible=True, value=alert_message),\n",
    "                gr.update(value=pdf_file_path), gr.update(value=pdf_file_path))\n",
    "    except Exception as e:\n",
    "        return (f\"Error: {str(e)}\", gr.update(visible=True, value=f\"Error: {str(e)}\"),\n",
    "                None, None)\n",
    "\n",
    "\n",
    "def image2b64(image_file):\n",
    "    with open(image_file, \"rb\") as f:\n",
    "        image_b64 = base64.b64encode(f.read()).decode()\n",
    "        return image_b64\n",
    "\n",
    "\n",
    "def save_chart(file_path: str, progress=gr.Progress()):\n",
    "    progress(0, desc=\"Starting task...\")\n",
    "    if not file_path:\n",
    "        return \"No file uploaded\", None, None\n",
    "\n",
    "    # Ensure the target directory exists\n",
    "    if not os.path.exists(TARGET_DIR_CHART):\n",
    "        os.makedirs(TARGET_DIR_CHART)\n",
    "\n",
    "    try:\n",
    "        # Get the file name\n",
    "        file_name = os.path.basename(file_path)\n",
    "        # Target file path\n",
    "        global cur_chart_dir\n",
    "        cur_chart_dir = os.path.join(TARGET_DIR_CHART, f\"{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\")\n",
    "        os.makedirs(cur_chart_dir, exist_ok=True)\n",
    "\n",
    "        gr.update(visible=True, value=\"waiting for chart description...\")\n",
    "\n",
    "        global chart_file_path\n",
    "        chart_file_path = os.path.join(cur_chart_dir, file_name)\n",
    "        print(\"chart_file_path:\")\n",
    "        print(chart_file_path)\n",
    "\n",
    "        # Copy the file to the target directory\n",
    "        shutil.copy(file_path, chart_file_path)\n",
    "        progress(0.20, desc=\"File uploaded successfully!\")\n",
    "\n",
    "        image_b64 = image2b64(chart_file_path)\n",
    "        chart_reading = ChatNVIDIA(model=\"microsoft/phi-3-vision-128k-instruct\")\n",
    "        result = chart_reading.invoke(\n",
    "            f'Generate underlying data table of the figure below, : <img src=\"data:image/png;base64,{image_b64}\" />')\n",
    "        print(result)\n",
    "\n",
    "        # Save result text to markdown file\n",
    "        global markdown_file_path_chart\n",
    "        markdown_file_path_chart = Path(f\"{cur_chart_dir}/chart.md\")\n",
    "        print(\"markdown_file_path_chart\")\n",
    "        print(markdown_file_path_chart)\n",
    "        with open(markdown_file_path_chart, \"w\") as f:\n",
    "            f.write(result.content)\n",
    "\n",
    "        progress(0.40, desc=\"Chart converted to markdown!\")\n",
    "        print(\"Chart converted to markdown!\" + str(markdown_file_path_chart))\n",
    "\n",
    "        # Read markdown content (assuming a function read_markdown_content exists)\n",
    "        data, sources = read_markdown_content(markdown_file_path_chart)\n",
    "\n",
    "        # Create FAISS store (assuming a function create_faiss_store exists)\n",
    "        global store_chart\n",
    "        store_chart = create_faiss_store(\n",
    "            documents=data,\n",
    "            sources=sources,\n",
    "            store_path=faiss_store_path_chart\n",
    "        )\n",
    "        progress(0.75, desc=\"FAISS store created successfully!\")\n",
    "\n",
    "        global retriever_chart\n",
    "        retriever_chart = store_chart.as_retriever()\n",
    "        progress(1, desc=\"Embedding and FAISS store created!\")\n",
    "\n",
    "        return gr.update(visible=True, value=result.content, lines=8)\n",
    "    except Exception as e:\n",
    "        return gr.update(visible=True, value=\"Error: \" + str(e))\n",
    "\n",
    "\n",
    "def article_agent(message: str, history: List[List[str]]) -> str:\n",
    "    \"\"\"\n",
    "    Process user messages and generate responses using the article chain.\n",
    "\n",
    "    Args:\n",
    "        message: User's input message\n",
    "        history: Chat history as list of [user_message, bot_message] pairs\n",
    "\n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article_chain, _, _ = create_chains()\n",
    "        response = article_chain.invoke(message)\n",
    "        # response = \"This is a response, you need to implement the response generation logic.\"\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error processing request: {str(e)}\"\n",
    "\n",
    "\n",
    "def respond(message, chat_history):\n",
    "    bot_message = article_agent(message, chat_history)\n",
    "    chat_history.append([message, bot_message])\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "def podcast_agent(message: str, progress=gr.Progress()) -> str:\n",
    "    try:\n",
    "        _, podcast_chain, _ = create_chains()\n",
    "        progress(0.20, desc=\"Podcast script chain builds successfully!\")\n",
    "        global podcast_text_plain\n",
    "        if message is None:\n",
    "            podcast_text_plain = podcast_chain.invoke(\n",
    "                \"Generate an English podcast script for an IELTS proficiency level.\")\n",
    "            progress(0.60, desc=\"Podcast script generated successfully!\")\n",
    "        else:\n",
    "            podcast_text_plain = podcast_chain.invoke(message)\n",
    "\n",
    "            podcast_text_plain = podcast_text_plain.strip()  # 去除首尾空格\n",
    "            podcast_text_plain = podcast_text_plain.replace(\"<podcast>\", \"\").replace(\"</podcast>\", \"\").strip()\n",
    "            podcast_storage_dir = Path(f\"data/podcast/{output_dir}\")\n",
    "            progress(0.80, desc=\"Podcast script generated successfully!\")\n",
    "            podcast_storage_dir.mkdir(parents=True, exist_ok=True)\n",
    "            with open(f\"./data/podcast/{output_dir}/podcast_script_text.txt\", \"w\") as f:\n",
    "                f.write(podcast_text_plain)\n",
    "            progress(1, desc=\"Podcast script saved successfully!\")\n",
    "\n",
    "        return podcast_text_plain\n",
    "    except Exception as e:\n",
    "        return f\"Error processing request: {str(e)}\"\n",
    "\n",
    "\n",
    "def podcast_audio_generate(progress=gr.Progress()):\n",
    "    global podcast_text_split\n",
    "    podcast_text_list = split_podcast_text(podcast_text_plain)\n",
    "\n",
    "    for i, text in enumerate(podcast_text_list):\n",
    "        progress(0.10 + i * 0.80 / len(podcast_text_list), desc=f\"Generating audio for paragraph {i + 1}...\")\n",
    "        synthesize_speech(text=text, voice=\"English-US.Male-Neutral\",\n",
    "                          output=f\"./data/podcast/{output_dir}/podcast_audio-{i}.wav\")\n",
    "\n",
    "    progress(0.8, desc=\"Audio files generated successfully!\")\n",
    "\n",
    "    combine_audio_files(f\"./data/podcast/{output_dir}\", f\"./data/podcast/{output_dir}/combined_podcast.wav\")\n",
    "    progress(1, desc=\"Audio files combined successfully!\")\n",
    "    return gr.update(visible=True, value=f\"./data/podcast/{output_dir}/combined_podcast.wav\")\n",
    "\n",
    "\n",
    "def chart_agent(message: str, progress=gr.Progress()):\n",
    "    _, _, chart_chain = create_chains()\n",
    "    if message is None:\n",
    "        chart_text_plain = chart_chain.invoke(\n",
    "            \"Generate an English writing for an IELTS proficiency level.\")\n",
    "        progress(0.60, desc=\"Chart script generated successfully!\")\n",
    "    else:\n",
    "        chart_text_plain = chart_chain.invoke(message)\n",
    "\n",
    "        chart_text_plain = chart_text_plain.strip()  # 去除首尾空格\n",
    "        chart_text_plain = chart_text_plain.replace(\"<essay>\", \"\").replace(\"</essay>\", \"\").strip()\n",
    "\n",
    "        progress(0.80, desc=\"Chart script generated successfully!\")\n",
    "\n",
    "        with open(f\"{cur_chart_dir}/chart_script_text.txt\", \"w\") as f:\n",
    "            f.write(chart_text_plain)\n",
    "        progress(1, desc=\"Chart script saved successfully!\")\n",
    "    return chart_text_plain"
   ],
   "id": "1e21d69ab103c6aa",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-23T06:46:24.075480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with gr.Blocks(theme=\"soft\") as demo:\n",
    "    with gr.Tab(label=\"Article Understanding\"):\n",
    "        gr.Markdown(\"# English Article Assistant\")\n",
    "        gr.Markdown(\"Upload an article and ask questions to understand it better.\")\n",
    "        with gr.Row():\n",
    "       \n",
    "                chatbot = gr.Chatbot(\n",
    "                    label=\"Chat History\",\n",
    "                    height=500,\n",
    "                    show_copy_button=True\n",
    "                )\n",
    "            \n",
    "\n",
    "        with gr.Row(scale=1):\n",
    "            with gr.Column(scale=3):\n",
    "                msg = gr.Textbox(\n",
    "                    label=\"Your Question\",\n",
    "                    placeholder=\"Ask about the article...\",\n",
    "                    lines=2\n",
    "                )\n",
    "\n",
    "            with gr.Column(scale=2):\n",
    "                send = gr.Button(\"Send\")\n",
    "                clear = gr.Button(\"Clear\")\n",
    "                \n",
    "            with gr.Column(scale=2):\n",
    "                article_file = gr.File(\n",
    "                    label=\"Upload PDF\",\n",
    "                    file_types=[\".pdf\"],\n",
    "                    scale=1,\n",
    "                    height=150\n",
    "                )\n",
    "                article_submit = gr.Button(\"Submit\", scale=1)\n",
    "        alert = gr.Textbox(visible=False, scale=2, lines=2, label=\"System message\")\n",
    "\n",
    "    with gr.Tab(label=\"Podcast Generate\"):\n",
    "        gr.Markdown(\"# English Podcast Generator\")\n",
    "        gr.Markdown(\"Generate a podcast script based on the uploaded article.\")\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                podcast_text = gr.Textbox(\n",
    "                    label=\"Podcast Script\",\n",
    "                    placeholder=\"Podcast script will appear here...\",\n",
    "                    lines=22,\n",
    "                )\n",
    "            with gr.Column(scale=2):\n",
    "                podcast_file = gr.File(\n",
    "                    label=\"Upload PDF\",\n",
    "                    file_types=[\".pdf\"],\n",
    "                    scale=1,\n",
    "                    height=150,\n",
    "\n",
    "                )\n",
    "                podcast_prompt_user = gr.Textbox(\n",
    "                    label=\"podcast_prompt_user\",\n",
    "                    value=\"Generate an English podcast script for an IELTS proficiency level.\",\n",
    "                    lines=2)\n",
    "                podcast_submit = gr.Button(\"Submit\", scale=1)\n",
    "                with gr.Row():\n",
    "                    podcast_script_text_generate = gr.Button(\"Generate Text\", scale=1)\n",
    "                    podcast_script_audio_generate = gr.Button(\"Generate Audio\", scale=1)\n",
    "                podcast_audio = gr.Audio(visible=True)\n",
    "    with gr.Tab(label=\"Chart&Table Description\"):\n",
    "        gr.Markdown(\"# Chart & Table Analyze\")\n",
    "        gr.Markdown(\"Upload a chart picture and generate an essay based on it.\")\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                picture_table_file = gr.Image(label=\"Upload a Chart\", scale=1, type=\"filepath\", height=300)\n",
    "\n",
    "                chart_description = gr.Textbox(label=\"Chart description\", lines=4, visible=True,\n",
    "                                               value=\"waiting for chart description...\")\n",
    "                with gr.Row():\n",
    "                    picture_table_submit = gr.Button(\"Submit Chart\", scale=1)\n",
    "                    chart_essay_generate = gr.Button(\"Generate Essay\", scale=1)\n",
    "            with gr.Column(scale=3):\n",
    "                chart_essay = gr.Textbox(label=\"Essay based on chart\", lines=18)\n",
    "                chart_user_input = gr.Textbox(label=\"chart_user_input\",\n",
    "                                              value=\"Generate an English writing for an IELTS proficiency level.\",\n",
    "                                              lines=2)\n",
    "\n",
    "    # 绑定按钮点击事件\n",
    "    article_submit.click(\n",
    "        fn=save_pdf,  # 处理文件的函数\n",
    "        inputs=article_file,  # 输入组件\n",
    "        outputs=[alert, podcast_file, article_file],  # 输出组件\n",
    "    )\n",
    "    podcast_submit.click(\n",
    "        fn=save_pdf,  # 处理文件的函数\n",
    "        inputs=podcast_file,  # 输入组件\n",
    "        outputs=[alert, podcast_file, article_file],  # 输出组件\n",
    "\n",
    "    )\n",
    "\n",
    "    send.click(\n",
    "        respond,\n",
    "        [msg, chatbot],\n",
    "        [msg, chatbot],\n",
    "        queue=False\n",
    "    )\n",
    "\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "    podcast_script_text_generate.click(\n",
    "        fn=podcast_agent,\n",
    "        inputs=podcast_prompt_user,\n",
    "        outputs=podcast_text\n",
    "    )\n",
    "\n",
    "    podcast_script_audio_generate.click(\n",
    "        fn=podcast_audio_generate,\n",
    "        inputs=None,\n",
    "        outputs=podcast_audio\n",
    "    )\n",
    "\n",
    "    picture_table_submit.click(\n",
    "        fn=save_chart,\n",
    "        inputs=picture_table_file,\n",
    "        outputs=[chart_description]\n",
    "    )\n",
    "\n",
    "    chart_essay_generate.click(\n",
    "        fn=chart_agent,\n",
    "        inputs=chart_user_input,\n",
    "        outputs=[chart_essay]\n",
    "    )\n",
    "\n",
    "demo.launch(demo, debug=True, share=False, server_port=15000)"
   ],
   "id": "5f5f208b1c7d66be",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LuckyE\\miniconda3\\envs\\RAG-Proj\\Lib\\site-packages\\gradio\\components\\chatbot.py:284: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:15000\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:27:46.258100Z",
     "start_time": "2025-02-23T06:27:46.256099Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f1ccfd57e311786a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
